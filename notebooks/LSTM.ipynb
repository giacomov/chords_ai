{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning music with a RNN - episode 2: the model\n",
    "\n",
    "In episode 1 we have gathered a set of around 5000 songs from the best 100 rock artists according to Rolling Stones. We now will train a Recurrent Neural Network on the sample and use it (in the next episode) to generate new sequences of chords.\n",
    "\n",
    "## Read the dataset\n",
    "\n",
    "Let's load the dataset we have generated in Episode 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import a bunch of things we will use\n",
    "\n",
    "import os\n",
    "os.environ['MKL_NUM_THREADS'] = '8'\n",
    "os.environ['OMP_NUM_THREADS'] = '8'\n",
    "\n",
    "# Force TensorFlow to run on the CPU (the GPU of my laptop is too slow for these things)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, BatchNormalization, Activation\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now read in the chord sequences for all the songs we have selected in Episode 1, as well as the vocabulary (i.e., the chords used in these songs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of songs: 5693\n",
      "Size of vocabulary: 68\n"
     ]
    }
   ],
   "source": [
    "# Read sequences and vocabulary\n",
    "# NOTE: the two JSON files used here are produced during Episode 1,\n",
    "# you need to run that first\n",
    "songs = pd.read_json(\"best_songs_cleaned.json\", typ='series')\n",
    "\n",
    "print(\"Number of songs: %s\" % len(songs))\n",
    "\n",
    "with open(\"best_songs_vocabulary.json\") as f:\n",
    "    \n",
    "    vocabulary = json.load(f)\n",
    "\n",
    "# Sort alphabetically\n",
    "vocabulary = sorted(vocabulary)\n",
    "\n",
    "print(\"Size of vocabulary: %i\" % len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also take a peek of what the data actually look like by printing the first few rows of our data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of chord sequences:\n",
      "0                                                 E A E A\n",
      "1       E D A E D A G# Bb A B Bb C B E A E B A B A E B...\n",
      "100     E C# A F# B E C# A F# B E C# A F# B E C# A F# ...\n",
      "1000    E A G#m C#m7 B E A E G#m E A E G#m E G#m E G#m...\n",
      "1001                        F Dm G F Dm G F Dm G F Dm G F\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Examples of chord sequences:\")\n",
    "print(songs.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we will use does not understand chords per se. Instead, we assign a integer to each chord using a mapping between the vocabulary and the integers.\n",
    "\n",
    "The network will then predict an integer instead of a chord. We will then reverse the mapping to obtain our predicted chord:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {k:v for k,v in zip(vocabulary, range(len(vocabulary)))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A character-level language model\n",
    "\n",
    "In order to achieve our goal we are going to train a character-level language model, where each \"character\" corresponds to a chord. In other words, our model will predict the next chord given a sequence of previous chords.\n",
    "\n",
    "So for example, let's consider the following sequence: \"C Am Dm G7 C C7 F C\". We will ask the network to predict the last \"C\" given the input sequence \"C Am Dm G7 C C7 F\". \n",
    "\n",
    "The length of the input sequence must be decided a priori. Using a short sequence will not give a lot of context to the network, which will then won't have much information. On the contrary, using a sequence too long will make the training difficult as the network will need more and more long term memory and the number of possible sequences will explode combinatorially. I found a good trade off using a input legnth of 8.\n",
    "\n",
    "In order to train the network we then need to divide our songs in input sequences `X` and expected outputs `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 257570\n",
      "Songs skipped: 265\n"
     ]
    }
   ],
   "source": [
    "# Now we need to generate sequences for each song\n",
    "\n",
    "# Length of the input sequence\n",
    "seq_length = 8\n",
    "\n",
    "sequences = []\n",
    "skipped = 0\n",
    "\n",
    "# Loop over the songs and accumulate sequences\n",
    "for i, song in enumerate(songs):\n",
    "    \n",
    "    # Split the song in chords, then assign the corresponding integer from\n",
    "    # the mapping\n",
    "    these_chords = map(lambda x:mapping[x], song.split())\n",
    "    \n",
    "    # Make sure there are no repetitions of the same chord (see Episode 1)\n",
    "    assert np.all(np.diff(these_chords)!=0)\n",
    "    \n",
    "    # A song needs to be seq_length + 1 long to be useful for our purposes,\n",
    "    # because we need an input sequence of length \"seq_length\" and an expected\n",
    "    # output (the other chord, i.e., the \"+1\"). If the song is shorter then\n",
    "    # seq_length + 1 we cannot use it\n",
    "    if len(these_chords) < seq_length + 1:\n",
    "\n",
    "        # Skip this song\n",
    "        skipped += 1\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # Let's accumulate all sequences contained in the song, \n",
    "        # without wrapping around the edge\n",
    "        for i in range(len(these_chords) - (seq_length + 1) + 1):\n",
    "\n",
    "            sequences.append(these_chords[i:i+seq_length+1])\n",
    "\n",
    "# Make sure all sequences are of the proper length\n",
    "l = map(lambda x:len(x), sequences)\n",
    "assert np.all(np.array(l)==seq_length+1)\n",
    "\n",
    "print(\"Number of sequences: %s\" % len(sequences))\n",
    "print(\"Songs skipped: %s\" % skipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now split our sequences in inputs (`X`) and output (`y`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the LSTM model\n",
    "\n",
    "For this project I will use the LSTM implementation of (Keras)[https://keras.io/]). \n",
    "\n",
    "Our RNN is made of 3 layers:\n",
    "\n",
    "1. Embedding layer: this layer maps each chord (represented as an integer) to a point in a dense n-dimensional space (called the \"embedding\" of the input). For example, let's say that \"C\" is represented by the integer 1. After the embedding layer, \"C\" will be instead represented by say the vector [0.5, 0.2, 0.3, 0.6]. Why doing this? Because during the training a mapping will be learned so that items with a similar function will be nearby in this space. For example, let's consider the chord of \"C\" major. Its (minor relative)[http://www.musiceducatorsinstitute.com/course/guitar/course3/M02S01_relative_chords.html] \"Am\" is going to be close by in the n-dimensional space because in many context they can be used together. We also expect to find close by the chords of the key of C major. Instead, chords of other keys should be further away. This helps the network learn the function of each chord in its context.\n",
    "2. Long Short Term Memory layer: this is a standard LSTM layer. We use a little bit of [dropout](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5) regularization to avoid overfitting.\n",
    "3. Dense layer: a normal fully-connected layer with a [softmax](https://en.wikipedia.org/wiki/Softmax_function) activation function. In order to make the training a little faster, we use (Batch Normalization)[https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c] here. Thanks to this dense layer the output of the RNN will be the probability for each of the chords in the vocabulary to be the next chord, given the input sequence.  We will use this information to make the behavior of our predictions a little more vary than just predicting always the same output for the same input (see Episode 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 8, 4)              272       \n",
      "_________________________________________________________________\n",
      "lstm1 (LSTM)                 (None, 200)               164000    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 68)                13668     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 68)                272       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 68)                0         \n",
      "=================================================================\n",
      "Total params: 178,212\n",
      "Trainable params: 178,076\n",
      "Non-trainable params: 136\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# We do not one-hot-encode because we will use the \n",
    "# sparse_categorical_crossentropy as loss function\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vocabulary), 4, input_length=seq_length))\n",
    "model.add(LSTM(200, dropout=0.1,\n",
    "               name='lstm1', \n",
    "               return_sequences=False))\n",
    "model.add(Dense(len(vocabulary)))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the progress of the model we introduce a custom metric, based on the `sparse_top_k_categorical_accuracy` implemented in Keras. Remember that the output of our network will be the probability for each of the chords in the dictionary of being the next chord, given an input sequence. The metric considers an outcome a success if one of the first `k` most probable values predicted by the network is the true value. In the normal accuracy measurement, instead, the outcome is considered a success only if the most probable value according to the network is the truth. We choose this metric because it helps to account for the fact that, given a sequence of chords, there is more than one \"correct\" possibility for the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metric and compile model\n",
    "from chords_ai.custom_metric import sparse_top_k_categorical_accuracy_3\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=\"adam\", metrics=[sparse_top_k_categorical_accuracy_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network\n",
    "\n",
    "Now we can train the network. But first we need to set aside a test set, so that we can evaluate the performances of the network on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Let's set aside 20% of the sequences, chosen randomly\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the fit then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 206056 samples, validate on 51514 samples\n",
      "Epoch 1/1000\n",
      "206056/206056 [==============================] - 57s 276us/step - loss: 2.8666 - sparse_top_k_categorical_accuracy_3: 0.5956 - val_loss: 3.0611 - val_sparse_top_k_categorical_accuracy_3: 0.5144\n",
      "Epoch 2/1000\n",
      "206056/206056 [==============================] - 57s 274us/step - loss: 2.3112 - sparse_top_k_categorical_accuracy_3: 0.6639 - val_loss: 2.5813 - val_sparse_top_k_categorical_accuracy_3: 0.5660\n",
      "Epoch 3/1000\n",
      "206056/206056 [==============================] - 56s 274us/step - loss: 2.1901 - sparse_top_k_categorical_accuracy_3: 0.6781 - val_loss: 2.2547 - val_sparse_top_k_categorical_accuracy_3: 0.6678\n",
      "Epoch 4/1000\n",
      "206056/206056 [==============================] - 52s 250us/step - loss: 2.1339 - sparse_top_k_categorical_accuracy_3: 0.6863 - val_loss: 2.1909 - val_sparse_top_k_categorical_accuracy_3: 0.6747\n",
      "Epoch 5/1000\n",
      "206056/206056 [==============================] - 52s 254us/step - loss: 2.0894 - sparse_top_k_categorical_accuracy_3: 0.6927 - val_loss: 2.0609 - val_sparse_top_k_categorical_accuracy_3: 0.7008\n",
      "Epoch 6/1000\n",
      "206056/206056 [==============================] - 54s 260us/step - loss: 2.0295 - sparse_top_k_categorical_accuracy_3: 0.7071 - val_loss: 1.9872 - val_sparse_top_k_categorical_accuracy_3: 0.7145\n",
      "Epoch 7/1000\n",
      "206056/206056 [==============================] - 58s 280us/step - loss: 1.9575 - sparse_top_k_categorical_accuracy_3: 0.7237 - val_loss: 1.9353 - val_sparse_top_k_categorical_accuracy_3: 0.7292\n",
      "Epoch 8/1000\n",
      "206056/206056 [==============================] - 60s 289us/step - loss: 1.8915 - sparse_top_k_categorical_accuracy_3: 0.7391 - val_loss: 1.8294 - val_sparse_top_k_categorical_accuracy_3: 0.7548\n",
      "Epoch 9/1000\n",
      "206056/206056 [==============================] - 63s 307us/step - loss: 1.8261 - sparse_top_k_categorical_accuracy_3: 0.7539 - val_loss: 1.7767 - val_sparse_top_k_categorical_accuracy_3: 0.7629\n",
      "Epoch 10/1000\n",
      "206056/206056 [==============================] - 81s 392us/step - loss: 1.7710 - sparse_top_k_categorical_accuracy_3: 0.7630 - val_loss: 1.7217 - val_sparse_top_k_categorical_accuracy_3: 0.7742\n",
      "Epoch 11/1000\n",
      "206056/206056 [==============================] - 68s 329us/step - loss: 1.7222 - sparse_top_k_categorical_accuracy_3: 0.7721 - val_loss: 1.6775 - val_sparse_top_k_categorical_accuracy_3: 0.7811\n",
      "Epoch 12/1000\n",
      "206056/206056 [==============================] - 58s 280us/step - loss: 1.6805 - sparse_top_k_categorical_accuracy_3: 0.7800 - val_loss: 1.6396 - val_sparse_top_k_categorical_accuracy_3: 0.7872\n",
      "Epoch 13/1000\n",
      "206056/206056 [==============================] - 58s 281us/step - loss: 1.6438 - sparse_top_k_categorical_accuracy_3: 0.7863 - val_loss: 1.6161 - val_sparse_top_k_categorical_accuracy_3: 0.7923\n",
      "Epoch 14/1000\n",
      "206056/206056 [==============================] - 59s 286us/step - loss: 1.6118 - sparse_top_k_categorical_accuracy_3: 0.7925 - val_loss: 1.5975 - val_sparse_top_k_categorical_accuracy_3: 0.7944\n",
      "Epoch 15/1000\n",
      "206056/206056 [==============================] - 58s 283us/step - loss: 1.5821 - sparse_top_k_categorical_accuracy_3: 0.7976 - val_loss: 1.5807 - val_sparse_top_k_categorical_accuracy_3: 0.7978\n",
      "Epoch 16/1000\n",
      "206056/206056 [==============================] - 54s 264us/step - loss: 1.5564 - sparse_top_k_categorical_accuracy_3: 0.8029 - val_loss: 1.5573 - val_sparse_top_k_categorical_accuracy_3: 0.8005\n",
      "Epoch 17/1000\n",
      "206056/206056 [==============================] - 49s 236us/step - loss: 1.5333 - sparse_top_k_categorical_accuracy_3: 0.8064 - val_loss: 1.5455 - val_sparse_top_k_categorical_accuracy_3: 0.8038\n",
      "Epoch 18/1000\n",
      "206056/206056 [==============================] - 52s 254us/step - loss: 1.5095 - sparse_top_k_categorical_accuracy_3: 0.8123 - val_loss: 1.5334 - val_sparse_top_k_categorical_accuracy_3: 0.8058\n",
      "Epoch 19/1000\n",
      "206056/206056 [==============================] - 50s 243us/step - loss: 1.4886 - sparse_top_k_categorical_accuracy_3: 0.8155 - val_loss: 1.5224 - val_sparse_top_k_categorical_accuracy_3: 0.8075\n",
      "Epoch 20/1000\n",
      "206056/206056 [==============================] - 50s 245us/step - loss: 1.4680 - sparse_top_k_categorical_accuracy_3: 0.8191 - val_loss: 1.5145 - val_sparse_top_k_categorical_accuracy_3: 0.8106\n",
      "Epoch 21/1000\n",
      "206056/206056 [==============================] - 56s 274us/step - loss: 1.4525 - sparse_top_k_categorical_accuracy_3: 0.8225 - val_loss: 1.5026 - val_sparse_top_k_categorical_accuracy_3: 0.8127\n",
      "Epoch 22/1000\n",
      "206056/206056 [==============================] - 55s 267us/step - loss: 1.4339 - sparse_top_k_categorical_accuracy_3: 0.8251 - val_loss: 1.4905 - val_sparse_top_k_categorical_accuracy_3: 0.8149\n",
      "Epoch 23/1000\n",
      "206056/206056 [==============================] - 55s 269us/step - loss: 1.4203 - sparse_top_k_categorical_accuracy_3: 0.8292 - val_loss: 1.4885 - val_sparse_top_k_categorical_accuracy_3: 0.8158\n",
      "Epoch 24/1000\n",
      "206056/206056 [==============================] - 65s 314us/step - loss: 1.4035 - sparse_top_k_categorical_accuracy_3: 0.8315 - val_loss: 1.4856 - val_sparse_top_k_categorical_accuracy_3: 0.8167\n",
      "Epoch 25/1000\n",
      "206056/206056 [==============================] - 51s 248us/step - loss: 1.3916 - sparse_top_k_categorical_accuracy_3: 0.8339 - val_loss: 1.4722 - val_sparse_top_k_categorical_accuracy_3: 0.8188\n",
      "Epoch 26/1000\n",
      "206056/206056 [==============================] - 49s 236us/step - loss: 1.3760 - sparse_top_k_categorical_accuracy_3: 0.8369 - val_loss: 1.4719 - val_sparse_top_k_categorical_accuracy_3: 0.8192\n",
      "Epoch 27/1000\n",
      "206056/206056 [==============================] - 58s 281us/step - loss: 1.3644 - sparse_top_k_categorical_accuracy_3: 0.8388 - val_loss: 1.4713 - val_sparse_top_k_categorical_accuracy_3: 0.8204\n",
      "Epoch 28/1000\n",
      "206056/206056 [==============================] - 68s 331us/step - loss: 1.3504 - sparse_top_k_categorical_accuracy_3: 0.8411 - val_loss: 1.4621 - val_sparse_top_k_categorical_accuracy_3: 0.8207\n",
      "Epoch 29/1000\n",
      "206056/206056 [==============================] - 67s 324us/step - loss: 1.3394 - sparse_top_k_categorical_accuracy_3: 0.8434 - val_loss: 1.4694 - val_sparse_top_k_categorical_accuracy_3: 0.8225\n",
      "Epoch 30/1000\n",
      "206056/206056 [==============================] - 58s 283us/step - loss: 1.3280 - sparse_top_k_categorical_accuracy_3: 0.8453 - val_loss: 1.4591 - val_sparse_top_k_categorical_accuracy_3: 0.8213\n",
      "Epoch 31/1000\n",
      "206056/206056 [==============================] - 63s 308us/step - loss: 1.3183 - sparse_top_k_categorical_accuracy_3: 0.8472 - val_loss: 1.4546 - val_sparse_top_k_categorical_accuracy_3: 0.8235\n",
      "Epoch 32/1000\n",
      "206056/206056 [==============================] - 57s 277us/step - loss: 1.3069 - sparse_top_k_categorical_accuracy_3: 0.8495 - val_loss: 1.4523 - val_sparse_top_k_categorical_accuracy_3: 0.8243\n",
      "Epoch 33/1000\n",
      "206056/206056 [==============================] - 53s 260us/step - loss: 1.2954 - sparse_top_k_categorical_accuracy_3: 0.8514 - val_loss: 1.4474 - val_sparse_top_k_categorical_accuracy_3: 0.8239\n",
      "Epoch 34/1000\n",
      "206056/206056 [==============================] - 56s 271us/step - loss: 1.2885 - sparse_top_k_categorical_accuracy_3: 0.8523 - val_loss: 1.4477 - val_sparse_top_k_categorical_accuracy_3: 0.8246\n",
      "Epoch 35/1000\n",
      "206056/206056 [==============================] - 62s 303us/step - loss: 1.2773 - sparse_top_k_categorical_accuracy_3: 0.8539 - val_loss: 1.4448 - val_sparse_top_k_categorical_accuracy_3: 0.8253\n",
      "Epoch 36/1000\n",
      "206056/206056 [==============================] - 63s 307us/step - loss: 1.2684 - sparse_top_k_categorical_accuracy_3: 0.8562 - val_loss: 1.4487 - val_sparse_top_k_categorical_accuracy_3: 0.8238\n",
      "Epoch 37/1000\n",
      "206056/206056 [==============================] - 54s 261us/step - loss: 1.2607 - sparse_top_k_categorical_accuracy_3: 0.8582 - val_loss: 1.4337 - val_sparse_top_k_categorical_accuracy_3: 0.8285\n",
      "Epoch 38/1000\n",
      "206056/206056 [==============================] - 54s 263us/step - loss: 1.2532 - sparse_top_k_categorical_accuracy_3: 0.8592 - val_loss: 1.4485 - val_sparse_top_k_categorical_accuracy_3: 0.8251\n",
      "Epoch 39/1000\n",
      "206056/206056 [==============================] - 50s 241us/step - loss: 1.2447 - sparse_top_k_categorical_accuracy_3: 0.8603 - val_loss: 1.4410 - val_sparse_top_k_categorical_accuracy_3: 0.8274\n",
      "Epoch 40/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206056/206056 [==============================] - 51s 247us/step - loss: 1.2389 - sparse_top_k_categorical_accuracy_3: 0.8612 - val_loss: 1.4406 - val_sparse_top_k_categorical_accuracy_3: 0.8268\n",
      "Epoch 41/1000\n",
      "206056/206056 [==============================] - 65s 314us/step - loss: 1.2319 - sparse_top_k_categorical_accuracy_3: 0.8628 - val_loss: 1.4327 - val_sparse_top_k_categorical_accuracy_3: 0.8265\n",
      "Epoch 42/1000\n",
      "206056/206056 [==============================] - 61s 297us/step - loss: 1.2223 - sparse_top_k_categorical_accuracy_3: 0.8646 - val_loss: 1.4326 - val_sparse_top_k_categorical_accuracy_3: 0.8293\n",
      "Epoch 43/1000\n",
      "206056/206056 [==============================] - 50s 241us/step - loss: 1.2176 - sparse_top_k_categorical_accuracy_3: 0.8660 - val_loss: 1.4378 - val_sparse_top_k_categorical_accuracy_3: 0.8288\n",
      "Epoch 44/1000\n",
      "206056/206056 [==============================] - 49s 239us/step - loss: 1.2054 - sparse_top_k_categorical_accuracy_3: 0.8671 - val_loss: 1.4344 - val_sparse_top_k_categorical_accuracy_3: 0.8283\n",
      "Epoch 45/1000\n",
      "206056/206056 [==============================] - 50s 244us/step - loss: 1.2033 - sparse_top_k_categorical_accuracy_3: 0.8681 - val_loss: 1.4359 - val_sparse_top_k_categorical_accuracy_3: 0.8281\n",
      "Epoch 46/1000\n",
      "206056/206056 [==============================] - 70s 339us/step - loss: 1.1947 - sparse_top_k_categorical_accuracy_3: 0.8698 - val_loss: 1.4368 - val_sparse_top_k_categorical_accuracy_3: 0.8281\n",
      "Epoch 47/1000\n",
      "206056/206056 [==============================] - 62s 301us/step - loss: 1.1908 - sparse_top_k_categorical_accuracy_3: 0.8700 - val_loss: 1.4381 - val_sparse_top_k_categorical_accuracy_3: 0.8281\n",
      "Epoch 48/1000\n",
      "206056/206056 [==============================] - 67s 324us/step - loss: 1.1853 - sparse_top_k_categorical_accuracy_3: 0.8717 - val_loss: 1.4287 - val_sparse_top_k_categorical_accuracy_3: 0.8314\n",
      "Epoch 49/1000\n",
      "206056/206056 [==============================] - 68s 330us/step - loss: 1.1811 - sparse_top_k_categorical_accuracy_3: 0.8717 - val_loss: 1.4303 - val_sparse_top_k_categorical_accuracy_3: 0.8305\n",
      "Epoch 50/1000\n",
      "206056/206056 [==============================] - 63s 303us/step - loss: 1.1744 - sparse_top_k_categorical_accuracy_3: 0.8729 - val_loss: 1.4350 - val_sparse_top_k_categorical_accuracy_3: 0.8287\n",
      "Epoch 51/1000\n",
      "206056/206056 [==============================] - 54s 263us/step - loss: 1.1717 - sparse_top_k_categorical_accuracy_3: 0.8737 - val_loss: 1.4331 - val_sparse_top_k_categorical_accuracy_3: 0.8292\n",
      "Epoch 52/1000\n",
      "206056/206056 [==============================] - 54s 262us/step - loss: 1.1627 - sparse_top_k_categorical_accuracy_3: 0.8753 - val_loss: 1.4393 - val_sparse_top_k_categorical_accuracy_3: 0.8293\n",
      "Epoch 53/1000\n",
      "206056/206056 [==============================] - 70s 342us/step - loss: 1.1577 - sparse_top_k_categorical_accuracy_3: 0.8766 - val_loss: 1.4353 - val_sparse_top_k_categorical_accuracy_3: 0.8292\n",
      "Epoch 54/1000\n",
      "206056/206056 [==============================] - 54s 262us/step - loss: 1.1535 - sparse_top_k_categorical_accuracy_3: 0.8762 - val_loss: 1.4390 - val_sparse_top_k_categorical_accuracy_3: 0.8279\n",
      "Epoch 55/1000\n",
      "206056/206056 [==============================] - 56s 274us/step - loss: 1.1467 - sparse_top_k_categorical_accuracy_3: 0.8784 - val_loss: 1.4479 - val_sparse_top_k_categorical_accuracy_3: 0.8273\n",
      "Epoch 56/1000\n",
      "206056/206056 [==============================] - 69s 335us/step - loss: 1.1438 - sparse_top_k_categorical_accuracy_3: 0.8787 - val_loss: 1.4386 - val_sparse_top_k_categorical_accuracy_3: 0.8289\n",
      "Epoch 57/1000\n",
      "206056/206056 [==============================] - 56s 270us/step - loss: 1.1409 - sparse_top_k_categorical_accuracy_3: 0.8788 - val_loss: 1.4417 - val_sparse_top_k_categorical_accuracy_3: 0.8272\n",
      "Epoch 58/1000\n",
      "206056/206056 [==============================] - 56s 270us/step - loss: 1.1369 - sparse_top_k_categorical_accuracy_3: 0.8800 - val_loss: 1.4399 - val_sparse_top_k_categorical_accuracy_3: 0.8290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcc33857050>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To avoid overtraining (aka memorization), we use early \n",
    "# stopping, i.e., keras stops the training when the accuracy \n",
    "# on the validation dataset stops improving.\n",
    "# We use patience=10, which means that keras will stop the\n",
    "# training after 10 epochs where the metrics did not improve.\n",
    "# This is needed because due to the random nature of the training\n",
    "# the metric could not improve for a few epochs and then jump\n",
    "# up, we do not want to stop too early.\n",
    "\n",
    "n_epochs = 1000 # We'll never reach 1000 epochs because \n",
    "                # of early stopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_sparse_top_k_categorical_accuracy_3', \n",
    "                               patience=10, \n",
    "                               verbose=0)\n",
    "\n",
    "# We use checkpointing, i.e., we keep track of the iteration with \n",
    "# the best accuracy and save it to a file.  Since there is some \n",
    "# randomness involved in the training, the val_acc could be at \n",
    "# its maximum not in the very last epoch.\n",
    "\n",
    "check_point = ModelCheckpoint(\"best_weights.h5\", \n",
    "                              monitor='val_sparse_top_k_categorical_accuracy_3', \n",
    "                              verbose=0, \n",
    "                              save_best_only=True, \n",
    "                              save_weights_only=True, \n",
    "                              mode='max')\n",
    "\n",
    "callbacks = [early_stopping, check_point]\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=256, verbose=1, shuffle=True,\n",
    "          initial_epoch=0, epochs=n_epochs,\n",
    "          validation_data=(X_test, y_test),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no point in continuing the training (actually we could have stopped earler). We can now save the model, which will then use in Episode 3 to generate new chord sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"best_weights.h5\")\n",
    "\n",
    "model.save(\"model_best_songs.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
